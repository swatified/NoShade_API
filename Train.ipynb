{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toxicity & Sentiment Analyzer Documentation\n",
    "\n",
    "This notebook provides a detailed explanation of a comprehensive text analysis system that combines toxicity detection and sentiment analysis. The system is designed to analyze text input for both toxic content (across multiple categories) and overall sentiment, making it useful for content moderation and text analysis applications.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The system consists of:\n",
    "- A ToxicityAnalyzer class that handles both toxicity and sentiment analysis\n",
    "- A machine learning pipeline using TF-IDF vectorization and logistic regression\n",
    "- Sentiment analysis using a pre-trained RoBERTa model\n",
    "- A Gradio-based web interface for easy interaction\n",
    "\n",
    "Let's break down each component and understand how they work together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required Libraries\n",
    "\n",
    "The code begins by importing necessary libraries:\n",
    "- Core data processing: `numpy`, `pandas`\n",
    "- Machine learning: `sklearn` components for text vectorization and classification\n",
    "- Deep learning: `transformers` for sentiment analysis\n",
    "- UI: `gradio` for the web interface\n",
    "- Utilities: `os`, `json`, `re`, `joblib`, `tqdm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "from transformers import pipeline\n",
    "import gradio as gr\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ToxicityAnalyzer Class\n",
    "\n",
    "### Initialization\n",
    "\n",
    "The `ToxicityAnalyzer` class is the core component of the system. The initialization method sets up:\n",
    "\n",
    "1. **Model Directory Structure**:\n",
    "   - Creates a directory for storing model artifacts\n",
    "   - Sets up paths for model, vectorizer, and sentiment cache files\n",
    "\n",
    "2. **Toxicity Categories**:\n",
    "   - Defines six categories: toxic, severe_toxic, obscene, threat, insult, identity_hate\n",
    "\n",
    "3. **Sentiment Analysis Pipeline**:\n",
    "   - Initializes the RoBERTa-based sentiment analyzer\n",
    "   - Sets up sentiment caching for performance optimization\n",
    "\n",
    "4. **Component Initialization**:\n",
    "   - Loads existing models if available\n",
    "   - Prepares for new model training if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToxicityAnalyzer:\n",
    "    def __init__(self, model_dir='model_artifacts'):\n",
    "        print(\"Initializing ToxicityAnalyzer...\")\n",
    "        self.model_dir = model_dir\n",
    "        os.makedirs(model_dir, exist_ok=True)\n",
    "        \n",
    "        self.model_path = os.path.join(model_dir, 'logistic_model.joblib')\n",
    "        self.vectorizer_path = os.path.join(model_dir, 'tfidf_vectorizer.joblib')\n",
    "        self.sentiment_cache_path = os.path.join(model_dir, 'sentiment_cache.json')\n",
    "        \n",
    "        # Label columns in order\n",
    "        self.label_columns = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "        \n",
    "        print(\"Loading sentiment analyzer...\")\n",
    "        self.sentiment_analyzer = pipeline(\n",
    "            \"sentiment-analysis\",\n",
    "            model=\"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "        )\n",
    "        \n",
    "        # Initialize components\n",
    "        self.sentiment_cache = self._load_sentiment_cache()\n",
    "        self.vectorizer = None\n",
    "        self.model = None\n",
    "        \n",
    "        # Load or train the model\n",
    "        self._initialize_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Cache Management\n",
    "\n",
    "The sentiment cache system improves performance by storing previously computed sentiment analyses:\n",
    "\n",
    "- `_load_sentiment_cache()`: Loads existing cache from disk\n",
    "- `_save_sentiment_cache()`: Persists current cache to disk\n",
    "\n",
    "This caching mechanism is particularly useful when processing large datasets with repeated text patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def _load_sentiment_cache(self):\n",
    "        if os.path.exists(self.sentiment_cache_path):\n",
    "            print(\"Loading sentiment cache...\")\n",
    "            with open(self.sentiment_cache_path, 'r') as f:\n",
    "                return json.load(f)\n",
    "        return {}\n",
    "\n",
    "    def _save_sentiment_cache(self):\n",
    "        with open(self.sentiment_cache_path, 'w') as f:\n",
    "            json.dump(self.sentiment_cache, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Processing and Sentiment Analysis\n",
    "\n",
    "Two key methods handle text preprocessing and sentiment analysis:\n",
    "\n",
    "1. **clean_text()**:\n",
    "   - Converts text to lowercase\n",
    "   - Removes special characters\n",
    "   - Normalizes the text for consistent processing\n",
    "\n",
    "2. **get_sentiment()**:\n",
    "   - Checks cache for existing results\n",
    "   - Applies RoBERTa model for sentiment analysis\n",
    "   - Maps results to a three-dimensional vector [negative, neutral, positive]\n",
    "   - Caches results for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def clean_text(self, text):\n",
    "        text = str(text).lower()\n",
    "        return re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "\n",
    "    def get_sentiment(self, text):\n",
    "        if text in self.sentiment_cache:\n",
    "            return self.sentiment_cache[text]\n",
    "        \n",
    "        result = self.sentiment_analyzer(text, truncation=True, max_length=128)\n",
    "        sentiment_map = {\n",
    "            'LABEL_0': [1, 0, 0],  # Negative\n",
    "            'LABEL_1': [0, 1, 0],  # Neutral\n",
    "            'LABEL_2': [0, 0, 1]   # Positive\n",
    "        }\n",
    "        sentiment = sentiment_map[result[0]['label']]\n",
    "        \n",
    "        self.sentiment_cache[text] = sentiment\n",
    "        return sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation Pipeline\n",
    "\n",
    "The `prepare_data` method implements a comprehensive feature engineering pipeline:\n",
    "\n",
    "1. **Text Preprocessing**:\n",
    "   - Cleans and normalizes input text\n",
    "   - Handles training vs. inference modes\n",
    "\n",
    "2. **Feature Extraction**:\n",
    "   - Creates TF-IDF features (up to 50,000 features)\n",
    "   - Extracts sentiment features\n",
    "   - Combines both feature sets\n",
    "\n",
    "3. **Training Mode Features**:\n",
    "   - Calculates label distribution\n",
    "   - Provides detailed progress information\n",
    "   - Returns both features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def prepare_data(self, df, training=False):\n",
    "        print(f\"\\nPreparing dataset with {len(df)} samples...\")\n",
    "        \n",
    "        # Extract features and labels\n",
    "        X = df['comment_text'].apply(self.clean_text)\n",
    "        if training:\n",
    "            y = df[self.label_columns].values\n",
    "            print(f\"Label distribution:\")\n",
    "            for col in self.label_columns:\n",
    "                positive_count = df[col].sum()\n",
    "                print(f\"{col}: {positive_count} positive samples ({positive_count/len(df)*100:.2f}%)\")\n",
    "        \n",
    "        print(\"\\nCreating TF-IDF features...\")\n",
    "        if training:\n",
    "            self.vectorizer = TfidfVectorizer(\n",
    "                max_features=50000,\n",
    "                ngram_range=(1, 2),\n",
    "                strip_accents='unicode',\n",
    "                min_df=5\n",
    "            )\n",
    "            X_tfidf = self.vectorizer.fit_transform(X)\n",
    "        else:\n",
    "            X_tfidf = self.vectorizer.transform(X)\n",
    "        \n",
    "        print(\"Getting sentiment features...\")\n",
    "        sentiment_features = []\n",
    "        for text in tqdm(X, desc=\"Processing sentiments\"):\n",
    "            sentiment = self.get_sentiment(text)\n",
    "            sentiment_features.append(sentiment)\n",
    "            \n",
    "        sentiment_features = np.array(sentiment_features)\n",
    "        \n",
    "        # Combine features\n",
    "        X_combined = sp.hstack([X_tfidf, sp.csr_matrix(sentiment_features)])\n",
    "        \n",
    "        if training:\n",
    "            return X_combined, y\n",
    "        return X_combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Initialization and Training\n",
    "\n",
    "The `_initialize_model` method handles the complete model lifecycle:\n",
    "\n",
    "1. **Data Loading**:\n",
    "   - Loads training data from CSV\n",
    "   - Handles existing model loading if available\n",
    "\n",
    "2. **Model Training**:\n",
    "   - Prepares features and labels\n",
    "   - Implements stratified splitting\n",
    "   - Trains separate classifiers for each toxicity category\n",
    "\n",
    "3. **Model Evaluation**:\n",
    "   - Calculates accuracy for each classifier\n",
    "   - Saves trained models for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def _initialize_model(self):\n",
    "        print(\"\\nLoading data...\")\n",
    "        df = pd.read_csv('data/train.csv')\n",
    "        print(f\"Dataset size: {len(df)} samples\")\n",
    "        \n",
    "        if os.path.exists(self.model_path) and os.path.exists(self.vectorizer_path):\n",
    "            print(\"Loading existing model and vectorizer...\")\n",
    "            self.vectorizer = joblib.load(self.vectorizer_path)\n",
    "            self.model = joblib.load(self.model_path)\n",
    "        else:\n",
    "            print(\"Training new model...\")\n",
    "            X_combined, y = self.prepare_data(df, training=True)\n",
    "            \n",
    "            print(\"\\nSplitting dataset...\")\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X_combined, y, test_size=0.2, random_state=42, \n",
    "                stratify=y[:, 0]  # Stratify on toxic label\n",
    "            )\n",
    "            \n",
    "            print(\"\\nTraining models for each label...\")\n",
    "            estimators = []\n",
    "            for i, label in enumerate(tqdm(self.label_columns)):\n",
    "                print(f\"\\nTraining classifier for {label}...\")\n",
    "                clf = LogisticRegression(\n",
    "                    C=1.0,\n",
    "                    max_iter=200,\n",
    "                    class_weight='balanced',\n",
    "                    verbose=1\n",
    "                )\n",
    "                clf.fit(X_train, y_train[:, i])\n",
    "                \n",
    "                # Evaluate on test set\n",
    "                score = clf.score(X_test, y_test[:, i])\n",
    "                print(f\"{label} classifier accuracy: {score:.4f}\")\n",
    "                estimators.append(clf)\n",
    "            \n",
    "            self.model = MultiOutputClassifier(estimators)\n",
    "            self.model.estimators_ = estimators\n",
    "            \n",
    "            print(\"\\nSaving models...\")\n",
    "            joblib.dump(self.model, self.model_path)\n",
    "            joblib.dump(self.vectorizer, self.vectorizer_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Analysis and Prediction\n",
    "\n",
    "The `score_comment` method is the main interface for analyzing individual texts. It combines all the preprocessing, feature extraction, and prediction steps into a single workflow:\n",
    "\n",
    "1. **Text Processing**:\n",
    "   - Takes raw text input\n",
    "   - Applies cleaning and normalization\n",
    "\n",
    "2. **Feature Generation**:\n",
    "   - Creates TF-IDF features from processed text\n",
    "   - Extracts sentiment features\n",
    "   - Combines features for prediction\n",
    "\n",
    "3. **Multi-label Classification**:\n",
    "   - Applies trained classifiers for each toxicity category\n",
    "   - Determines the dominant sentiment\n",
    "   - Formats results for user consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def score_comment(self, comment):\n",
    "        cleaned_text = self.clean_text(comment)\n",
    "        \n",
    "        # Get features\n",
    "        X_tfidf = self.vectorizer.transform([cleaned_text])\n",
    "        sentiment_features = np.array([self.get_sentiment(cleaned_text)])\n",
    "        X_combined = sp.hstack([X_tfidf, sp.csr_matrix(sentiment_features)])\n",
    "        \n",
    "        # Get predictions\n",
    "        predictions = [clf.predict(X_combined)[0] for clf in self.model.estimators_]\n",
    "        \n",
    "        # Format sentiment output\n",
    "        sentiment_labels = ['Negative', 'Neutral', 'Positive']\n",
    "        dominant_sentiment = sentiment_labels[np.argmax(sentiment_features[0])]\n",
    "        \n",
    "        # Get detected labels\n",
    "        if dominant_sentiment == 'Negative':\n",
    "            predictions = [clf.predict(X_combined)[0] for clf in self.model.estimators_]\n",
    "            detected_labels = []\n",
    "            for label, pred in zip(self.label_columns, predictions):\n",
    "                if pred == 1:\n",
    "                    detected_labels.append(label.replace('_', ' ').title())\n",
    "            return dominant_sentiment, \"\\n\".join(detected_labels) if detected_labels else \"No toxic labels detected\"\n",
    "        else:\n",
    "            return dominant_sentiment, \"No toxic labels detected\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Interface Implementation\n",
    "\n",
    "The final component of the system is a web-based user interface built using Gradio. This interface makes the toxicity analyzer accessible through a browser, with the following features:\n",
    "\n",
    "1. **Input Interface**:\n",
    "   - Text input box with 3 lines\n",
    "   - Clear placeholder text\n",
    "   - User-friendly layout\n",
    "\n",
    "2. **Output Display**:\n",
    "   - Sentiment result field\n",
    "   - Detected toxicity labels field\n",
    "   - Clean, organized presentation\n",
    "\n",
    "3. **Styling and Theme**:\n",
    "   - Dark mode interface\n",
    "   - Blue accent colors\n",
    "   - Professional appearance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_interface():\n",
    "    analyzer = ToxicityAnalyzer()\n",
    "    \n",
    "    interface = gr.Interface(\n",
    "        fn=analyzer.score_comment,\n",
    "        inputs=gr.Textbox(\n",
    "            lines=3, \n",
    "            placeholder='Type your text here...',\n",
    "            label=\"Input Text\"\n",
    "        ),\n",
    "        outputs=[\n",
    "            gr.Textbox(label=\"Sentiment\", lines=1),\n",
    "            gr.Textbox(label=\"Detected Labels\", lines=6)\n",
    "        ],\n",
    "        title=\"Toxicity & Sentiment Analyzer\",\n",
    "        description=\"Analysis of text for toxicity and sentiment\",\n",
    "        theme=gr.themes.Base(primary_hue=\"blue\", neutral_hue=\"slate\"),\n",
    "        css=\"\"\"\n",
    "            .gradio-container {background-color: #1f1f1f}\n",
    "        \"\"\"\n",
    "    )\n",
    "    return interface\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting Toxicity Analyzer...\")\n",
    "    interface = create_interface()\n",
    "    interface.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage Example\n",
    "\n",
    "To use the Toxicity Analyzer:\n",
    "\n",
    "1. Ensure all required libraries are installed\n",
    "2. Run the script to start the web interface\n",
    "3. Enter text in the input box\n",
    "4. View sentiment and toxicity results\n",
    "\n",
    "The system will automatically handle model loading/training and provide real-time analysis of input text."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
