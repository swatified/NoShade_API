{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toxicity & Sentiment Analyzer Documentation\n",
    "\n",
    "This notebook provides a detailed explanation of a comprehensive text analysis system that combines toxicity detection and sentiment analysis. The system is designed to analyze text input for both toxic content (across multiple categories) and overall sentiment, making it useful for content moderation and text analysis applications.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The system consists of:\n",
    "- A ToxicityAnalyzer class that handles both toxicity and sentiment analysis\n",
    "- A machine learning pipeline using TF-IDF vectorization and logistic regression\n",
    "- Sentiment analysis using a pre-trained RoBERTa model\n",
    "- A Gradio-based web interface for easy interaction\n",
    "\n",
    "Let's break down each component and understand how they work together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required Libraries\n",
    "\n",
    "The code begins by importing necessary libraries:\n",
    "- Core data processing: `numpy`, `pandas`\n",
    "- Machine learning: `sklearn` components for text vectorization and classification\n",
    "- Deep learning: `transformers` for sentiment analysis\n",
    "- UI: `gradio` for the web interface\n",
    "- Utilities: `os`, `json`, `re`, `joblib`, `tqdm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "from transformers import pipeline\n",
    "import gradio as gr\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ToxicityAnalyzer Class\n",
    "\n",
    "### Initialization\n",
    "The `__init__` method sets up the analyzer with:\n",
    "1. Directory structure for model artifacts\n",
    "2. Paths for model, vectorizer, and sentiment cache\n",
    "3. Label definitions for toxic categories\n",
    "4. Pre-trained sentiment analysis pipeline\n",
    "5. Component initialization\n",
    "\n",
    "Key features:\n",
    "- Uses the Cardiff NLP RoBERTa model for sentiment analysis\n",
    "- Maintains a cache for sentiment analysis results\n",
    "- Supports six different toxicity categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToxicityAnalyzer:\n",
    "    def __init__(self, model_dir='model_artifacts'):\n",
    "        print(\"Initializing ToxicityAnalyzer...\")\n",
    "        self.model_dir = model_dir\n",
    "        os.makedirs(model_dir, exist_ok=True)\n",
    "        \n",
    "        self.model_path = os.path.join(model_dir, 'logistic_model.joblib')\n",
    "        self.vectorizer_path = os.path.join(model_dir, 'tfidf_vectorizer.joblib')\n",
    "        self.sentiment_cache_path = os.path.join(model_dir, 'sentiment_cache.json')\n",
    "        \n",
    "        # Label columns in order\n",
    "        self.label_columns = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "        \n",
    "        print(\"Loading sentiment analyzer...\")\n",
    "        self.sentiment_analyzer = pipeline(\n",
    "            \"sentiment-analysis\",\n",
    "            model=\"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "        )\n",
    "        \n",
    "        # Initialize components\n",
    "        self.sentiment_cache = self._load_sentiment_cache()\n",
    "        self.vectorizer = None\n",
    "        self.model = None\n",
    "        \n",
    "        # Load or train the model\n",
    "        self._initialize_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Cache Management\n",
    "\n",
    "These methods handle the sentiment analysis cache:\n",
    "- `_load_sentiment_cache`: Loads previously cached sentiment results\n",
    "- `_save_sentiment_cache`: Saves current sentiment results to cache\n",
    "\n",
    "The cache system improves performance by avoiding redundant sentiment analysis on previously processed text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def _load_sentiment_cache(self):\n",
    "        if os.path.exists(self.sentiment_cache_path):\n",
    "            print(\"Loading sentiment cache...\")\n",
    "            with open(self.sentiment_cache_path, 'r') as f:\n",
    "                return json.load(f)\n",
    "        return {}\n",
    "\n",
    "    def _save_sentiment_cache(self):\n",
    "        with open(self.sentiment_cache_path, 'w') as f:\n",
    "            json.dump(self.sentiment_cache, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Processing Methods\n",
    "\n",
    "These methods handle text preprocessing and sentiment analysis:\n",
    "- `clean_text`: Normalizes text by converting to lowercase and removing special characters\n",
    "- `get_sentiment`: Performs sentiment analysis and maps results to a three-dimensional vector\n",
    "\n",
    "The sentiment analysis uses a three-way classification:\n",
    "- [1,0,0]: Negative\n",
    "- [0,1,0]: Neutral\n",
    "- [0,0,1]: Positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def clean_text(self, text):\n",
    "        text = str(text).lower()\n",
    "        return re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "\n",
    "    def get_sentiment(self, text):\n",
    "        if text in self.sentiment_cache:\n",
    "            return self.sentiment_cache[text]\n",
    "        \n",
    "        result = self.sentiment_analyzer(text, truncation=True, max_length=128)\n",
    "        sentiment_map = {\n",
    "            'LABEL_0': [1, 0, 0],  # Negative\n",
    "            'LABEL_1': [0, 1, 0],  # Neutral\n",
    "            'LABEL_2': [0, 0, 1]   # Positive\n",
    "        }\n",
    "        sentiment = sentiment_map[result[0]['label']]\n",
    "        \n",
    "        self.sentiment_cache[text] = sentiment\n",
    "        return sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "The `prepare_data` method handles feature engineering:\n",
    "1. Text cleaning and preprocessing\n",
    "2. TF-IDF feature extraction\n",
    "3. Sentiment feature extraction\n",
    "4. Feature combination\n",
    "\n",
    "Key features:\n",
    "- Uses both TF-IDF and sentiment features\n",
    "- Supports both training and inference modes\n",
    "- Provides detailed progress information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def prepare_data(self, df, training=False):\n",
    "        print(f\"\\nPreparing dataset with {len(df)} samples...\")\n",
    "        \n",
    "        # Extract features and labels\n",
    "        X = df['comment_text'].apply(self.clean_text)\n",
    "        if training:\n",
    "            y = df[self.label_columns].values\n",
    "            print(f\"Label distribution:\")\n",
    "            for col in self.label_columns:\n",
    "                positive_count = df[col].sum()\n",
    "                print(f\"{col}: {positive_count} positive samples ({positive_count/len(df)*100:.2f}%)\")\n",
    "        \n",
    "        print(\"\\nCreating TF-IDF features...\")\n",
    "        if training:\n",
    "            self.vectorizer = TfidfVectorizer(\n",
    "                max_features=50000,\n",
    "                ngram_range=(1, 2),\n",
    "                strip_accents='unicode',\n",
    "                min_df=5\n",
    "            )\n",
    "            X_tfidf = self.vectorizer.fit_transform(X)\n",
    "        else:\n",
    "            X_tfidf = self.vectorizer.transform(X)\n",
    "        \n",
    "        print(\"Getting sentiment features...\")\n",
    "        sentiment_features = []\n",
    "        for text in tqdm(X, desc=\"Processing sentiments\"):\n",
    "            sentiment = self.get_sentiment(text)\n",
    "            sentiment_features.append(sentiment)\n",
    "            \n",
    "        sentiment_features = np.array(sentiment_features)\n",
    "        \n",
    "        # Combine features\n",
    "        X_combined = sp.hstack([X_tfidf, sp.csr_matrix(sentiment_features)])\n",
    "        \n",
    "        if training:\n",
    "            return X_combined, y\n",
    "        return X_combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Initialization and Training\n",
    "\n",
    "The `_initialize_model` method handles model setup:\n",
    "1. Loads or creates training data\n",
    "2. Prepares features and labels\n",
    "3. Trains separate classifiers for each toxicity category\n",
    "4. Evaluates and saves the models\n",
    "\n",
    "Key features:\n",
    "- Uses balanced class weights for handling imbalanced data\n",
    "- Implements stratified splitting for better representation\n",
    "- Saves trained models for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def _initialize_model(self):\n",
    "        print(\"\\nLoading data...\")\n",
    "        df = pd.read_csv('data/train.csv')\n",
    "        print(f\"Dataset size: {len(df)} samples\")\n",
    "        \n",
    "        if os.path.exists(self.model_path) and os.path.exists(self.vectorizer_path):\n",
    "            print(\"Loading existing model and vectorizer...\")\n",
    "            self.vectorizer = joblib.load(self.vectorizer_path)\n",
    "            self.model = joblib.load(self.model_path)\n",
    "        else:\n",
    "            print(\"Training new model...\")\n",
    "            X_combined, y = self.prepare_data(df, training=True)\n",
    "            \n",
    "            print(\"\\nSplitting dataset...\")\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X_combined, y, test_size=0.2, random_state=42, \n",
    "                stratify=y[:, 0]  # Stratify on toxic label\n",
    "            )\n",
    "            \n",
    "            print(\"\\nTraining models for each label...\")\n",
    "            estimators = []\n",
    "            for i, label in enumerate(tqdm(self.label_columns)):\n",
    "                print(f\"\\nTraining classifier for {label}...\")\n",
    "                clf = LogisticRegression(\n",
    "                    C=1.0,\n",
    "                    max_iter=200,\n",
    "                    class_weight='balanced',\n",
    "                    verbose=1\n",
    "                )\n",
    "                clf.fit(X_train, y_train[:, i])\n",
    "                \n",
    "                # Evaluate on test set\n",
    "                score = clf.score(X_test, y_test[:, i])\n",
    "                print(f\"{label} classifier accuracy: {score:.4f}\")\n",
    "                estimators.append(clf)\n",
    "            \n",
    "            self.model = MultiOutputClassifier(estimators)\n",
    "            self.model.estimators_ = estimators\n",
    "            \n",
    "            print(\"\\nSaving models...\")\n",
    "            joblib.dump(self.model, self.model_path)\n",
    "            joblib.dump(self.vectorizer, self.vectorizer_path
